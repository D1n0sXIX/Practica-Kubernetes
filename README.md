# Pr√°ctica 2: Despliegue de aplicaciones usando Kubernetes/Docker
## Sistemas Distribuidos - Curso 2024/2025
## Alejandro Mam√°n L√≥pez-Mingo
---

## üìã √çndice

1. [Descripci√≥n del Proyecto](#descripci√≥n-del-proyecto)
2. [Arquitectura Implementada](#arquitectura-implementada)
3. [Configuraci√≥n B√°sica (COMPLETADA)](#configuraci√≥n-b√°sica-completada)
4. [Pasos para Llegar al 10](#pasos-para-llegar-al-10)
5. [Comandos √ötiles](#comandos-√∫tiles)
6. [Troubleshooting](#troubleshooting)

---

## üìñ Descripci√≥n del Proyecto

Sistema distribuido de gesti√≥n de archivos desplegado en un cluster Kubernetes con:
- **Broker:** Coordina las conexiones entre clientes y servidores
- **Servidores:** Gestionan archivos remotos (listado, subida, descarga)
- **Cliente:** Interfaz para interactuar con el sistema

### Componentes proporcionados:
- `brokerFileManager` - Ejecutable del broker
- `serverFileManager` - Ejecutable del servidor
- `clientFileManager` - Ejecutable del cliente

---

## üèóÔ∏è Arquitectura Implementada

### Cluster Kubernetes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NODO: k8smaster0.psdi.org (control-plane)                  ‚îÇ
‚îÇ  IP: 172.31.64.84                                           ‚îÇ
‚îÇ  Rol: Master + Puede ejecutar pods                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NODO: k8sslave1.psdi.org (worker)                          ‚îÇ
‚îÇ  IP: 172.31.31.30                                           ‚îÇ
‚îÇ  Label: node-role=broker                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  POD: broker-deployment                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Imagen: d1n0s/kubernetes-practica2broker:v1         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Puerto: 32002                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NODO: k8sslave2.psdi.org (worker)                          ‚îÇ
‚îÇ  IP: 172.31.72.209                                          ‚îÇ
‚îÇ  Label: node-role=server                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  POD: server-deployment                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Imagen: d1n0s/kubernetes-practica2server:v2         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Puerto: 32001                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Directorio: FileManagerDir/                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Servicios Kubernetes

| Servicio | Tipo | ClusterIP | NodePort | Selector |
|----------|------|-----------|----------|----------|
| brokerservice | NodePort | 10.96.11.73 | 32002 | app=brokerfilemanager |
| serverservice | NodePort | 10.106.134.34 | 32001 | app=server-deploy |

---

## ‚úÖ Configuraci√≥n B√°sica (COMPLETADA)

### 1. Im√°genes Docker Creadas

#### Imagen del Broker
```dockerfile
FROM ubuntu:20.04
RUN apt-get update
RUN apt-get install -y software-properties-common curl
EXPOSE 32002
COPY brokerFileManager /
RUN chmod +x /brokerFileManager
CMD /brokerFileManager
```

**Imagen en Docker Hub:** `d1n0s/kubernetes-practica2broker:v1`

#### Imagen del Servidor
```dockerfile
FROM ubuntu:20.04
RUN apt-get update
RUN apt-get install -y software-properties-common curl
EXPOSE 32001
COPY serverFileManager /
RUN chmod +x /serverFileManager
RUN mkdir FileManagerDir
COPY resolv.conf /
CMD cp resolv.conf /etc/resolv.conf && /serverFileManager 172.31.31.30 32002 $(curl -s https://api.ipify.org) 32001
```

**Imagen en Docker Hub:** `d1n0s/kubernetes-practica2server:v2`

**Nota:** El servidor se conecta al broker usando la IP `172.31.31.30` (k8sslave1)

### 2. Cluster Kubernetes Configurado

```bash
# Estado del cluster
kubectl get nodes -o wide
```

**Resultado:**
```
NAME                  STATUS   ROLES           AGE   VERSION    INTERNAL-IP
k8smaster0.psdi.org   Ready    control-plane   15d   v1.28.15   172.31.64.84
k8sslave1.psdi.org    Ready    worker          49m   v1.28.15   172.31.31.30
k8sslave2.psdi.org    Ready    worker          2m    v1.28.15   172.31.72.209
```

### 3. Deployments y Services Aplicados

#### Broker Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: broker-deployment
spec:
 replicas: 1
 selector:
  matchLabels:
   app: brokerfilemanager
 template:
  metadata:
   labels:
    app: brokerfilemanager
  spec:
   nodeSelector:
    node-role: broker
   containers:
   - name: broker-deployment
     image: docker.io/d1n0s/kubernetes-practica2broker:v1
     ports:
     - containerPort: 32002
```

#### Server Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: server-deployment
spec:
 replicas: 1
 selector:
  matchLabels:
   app: server-deploy
 template:
  metadata:
   labels:
    app: server-deploy
  spec:
   nodeSelector:
    node-role: server
   containers:
   - name: server-deployment
     image: docker.io/d1n0s/kubernetes-practica2server:v2
     ports:
     - containerPort: 32001
```

### 4. Verificaci√≥n del Sistema

```bash
# Ver pods
kubectl get pods -o wide
```

**Resultado:**
```
NAME                                 READY   STATUS    NODE
broker-deployment-6fd556654c-jzdsx   1/1     Running   k8sslave1.psdi.org
server-deployment-689b756d6-6jqz8    1/1     Running   k8sslave2.psdi.org
```

### 5. Prueba del Cliente

```bash
# Conectar al broker (IP privada dentro del cluster)
./clientFileManager 172.31.31.30 32002
```

**Comandos disponibles:**
- `ls` - Lista archivos locales al cliente
- `lls` - Lista archivos en FileManagerDir/ del servidor
- `upload archivo.txt` - Sube archivo al servidor
- `download archivo.txt` - Descarga archivo del servidor
- `exit` - Cierra la conexi√≥n

**‚úÖ CONFIGURACI√ìN B√ÅSICA APROBADA**

---

## üéØ Pasos para Llegar al 10

Para obtener la m√°xima calificaci√≥n, debes implementar una de las dos configuraciones avanzadas:

**‚úÖ OPCI√ìN SELECCIONADA: Opci√≥n 2 - M√∫ltiples Nodos con NFS Compartido**

### üìå Opci√≥n 1: M√∫ltiples R√©plicas con Volumen Compartido (hostPath)

*Esta opci√≥n NO ha sido implementada. Se ha seleccionado la Opci√≥n 2.*

Esta configuraci√≥n permite tener **m√∫ltiples r√©plicas del servidor en un mismo nodo** compartiendo la misma carpeta de archivos.

#### Paso 1: Crear PersistentVolume con hostPath

Crea el archivo `pv-hostpath.yml`:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: server-pv-hostpath
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /mnt/filemanager-data
    type: DirectoryOrCreate
  storageClassName: manual
```

#### Paso 2: Crear PersistentVolumeClaim

Crea el archivo `pvc-hostpath.yml`:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: server-pvc-hostpath
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: manual
```

#### Paso 3: Modificar DeploymentServer.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: server-deployment
spec:
 replicas: 3  # ‚Üê Cambiar a 3 r√©plicas
 selector:
  matchLabels:
   app: server-deploy
 template:
  metadata:
   labels:
    app: server-deploy
  spec:
   nodeSelector:
    node-role: server  # Todas las r√©plicas en k8sslave2
   containers:
   - name: server-deployment
     image: docker.io/d1n0s/kubernetes-practica2server:v2
     ports:
     - containerPort: 32001
     volumeMounts:  # ‚Üê A√±adir esto
     - name: filemanager-storage
       mountPath: /FileManagerDir
   volumes:  # ‚Üê A√±adir esto
   - name: filemanager-storage
     persistentVolumeClaim:
       claimName: server-pvc-hostpath
```

#### Paso 4: Aplicar los cambios

```bash
# Aplicar PV y PVC
kubectl apply -f ~/Practica2/SERVER/pv-hostpath.yml
kubectl apply -f ~/Practica2/SERVER/pvc-hostpath.yml

# Verificar
kubectl get pv
kubectl get pvc

# Redesplegar servidor
kubectl delete deployment server-deployment
kubectl apply -f ~/Practica2/SERVER/DeploymentServer.yml

# Verificar que hay 3 r√©plicas
kubectl get pods -o wide
```

#### Paso 5: Probar persistencia

```bash
# Conectar al cliente
./clientFileManager 172.31.31.30 32002

# Subir un archivo
upload test.txt

# Listar archivos en el servidor
lls

# Salir y volver a conectar
exit
./clientFileManager 172.31.31.30 32002

# El archivo debe seguir ah√≠
lls
```

**Ventaja:** Las 3 r√©plicas comparten los mismos archivos. Si subes un archivo conect√°ndote a una r√©plica, las otras tambi√©n lo ver√°n.

---

### üìå Opci√≥n 2: M√∫ltiples Nodos con NFS Compartido ‚≠ê **(IMPLEMENTANDO)**

Esta configuraci√≥n permite tener **servidores distribuidos en m√∫ltiples nodos** compartiendo archivos mediante NFS.

#### Requisitos previos
- ‚ö†Ô∏è A√±adir un tercer nodo worker (k8sslave3) al cluster *(Opcional para la demostraci√≥n)*
- Etiquetar k8sslave2 como `node-role=server`

#### Paso 1: Instalar servidor NFS en k8smaster0 ‚úÖ **COMPLETADO**

```bash
# En k8smaster0
sudo apt-get update
sudo apt-get install -y nfs-kernel-server

# Crear directorio compartido
sudo mkdir -p /mnt/nfs-filemanager
sudo chown nobody:nogroup /mnt/nfs-filemanager
sudo chmod 777 /mnt/nfs-filemanager

# Configurar exports (SIGUIENTE PASO)
echo "/mnt/nfs-filemanager *(rw,sync,no_subtree_check,no_root_squash)" | sudo tee -a /etc/exports

# Reiniciar NFS (SIGUIENTE PASO)
sudo exportfs -ra
sudo systemctl restart nfs-kernel-server
```

**‚úÖ Progreso actual:**
- ‚úÖ Paquete `nfs-kernel-server` instalado correctamente
- ‚úÖ Directorio `/mnt/nfs-filemanager` creado
- ‚úÖ Permisos configurados (nobody:nogroup, 777)
- ‚úÖ Archivo `/etc/exports` configurado
- ‚úÖ Servicio NFS activo y exportando el directorio

**‚ö†Ô∏è Problema encontrado y resuelto:**
- Error: Faltaba `/` inicial en la ruta (`mnt/nfs-filemanager` ‚Üí `/mnt/nfs-filemanager`)
- Soluci√≥n: Corregido con `sed` y re-aplicado correctamente

#### Paso 2: Instalar cliente NFS en workers ‚úÖ **COMPLETADO**

```bash
# Desde k8smaster0, instalar en k8sslave2
kubectl debug node/k8sslave2.psdi.org -it --image=ubuntu -- chroot /host bash
apt-get update
apt-get install -y nfs-common
exit
```

**‚úÖ Verificaci√≥n completada:**
```bash
kubectl debug node/k8sslave2.psdi.org -it --image=ubuntu -- chroot /host bash -c "dpkg -l | grep nfs"
# Resultado: nfs-common 1:2.6.1-1ubuntu1.2 instalado correctamente
```

#### Paso 3: Crear PersistentVolume NFS ‚úÖ **COMPLETADO**

Archivo `pv-nfs.yml`:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: server-pv-nfs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.64.84  # IP del k8smaster0
    path: /mnt/nfs-filemanager
  storageClassName: nfs
```

**‚úÖ Aplicado correctamente:**
```bash
kubectl apply -f pv-nfs.yml
# persistentvolume/server-pv-nfs created
```

#### Paso 4: Crear PersistentVolumeClaim NFS ‚úÖ **COMPLETADO**

Archivo `pvc-nfs.yml`:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: server-pvc-nfs
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: nfs
```

**‚úÖ Aplicado y vinculado correctamente:**
```bash
kubectl apply -f pvc-nfs.yml
# persistentvolumeclaim/server-pvc-nfs created

kubectl get pv
# NAME            CAPACITY   ACCESS MODES   STATUS   CLAIM
# server-pv-nfs   5Gi        RWX            Bound    default/server-pvc-nfs

kubectl get pvc
# NAME             STATUS   VOLUME          CAPACITY   ACCESS MODES
# server-pvc-nfs   Bound    server-pv-nfs   5Gi        RWX
```

**‚úÖ Estado: PV y PVC correctamente vinculados (Bound)**

#### Paso 5: Modificar DeploymentServer.yml ‚úÖ **COMPLETADO**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: server-deployment
spec:
 replicas: 3  # ‚Üê 3 r√©plicas distribuidas
 selector:
  matchLabels:
   app: server-deploy
 template:
  metadata:
   labels:
    app: server-deploy
  spec:
   nodeSelector:
    node-role: server  # Todas en k8sslave2 (solo tenemos 1 nodo worker con label server)
   containers:
   - name: server-deployment
     image: docker.io/d1n0s/kubernetes-practica2server:v2
     ports:
     - containerPort: 32001
     volumeMounts:
     - name: filemanager-storage-nfs
       mountPath: /FileManagerDir
   volumes:
   - name: filemanager-storage-nfs
     persistentVolumeClaim:
       claimName: server-pvc-nfs
```

**‚ö†Ô∏è Problemas encontrados y resueltos:**

1. **Mount timeout (Connection timed out)**
   - **Causa:** Security Group de AWS bloqueando puertos NFS
   - **Soluci√≥n:** A√±adidas reglas de entrada en Security Group:
     - Puerto 2049 (NFS) desde 172.31.0.0/16
     - Puerto 111 (RPC) desde 172.31.0.0/16

2. **ImagePullBackOff**
   - **Causa:** Versi√≥n de imagen incorrecta (v3 no existe)
   - **Soluci√≥n:** Corregido a v2 en DeploymentServer.yml

**‚úÖ Estado actual:**
```bash
kubectl get pods -o wide
# NAME                                 READY   STATUS    AGE
# broker-deployment-6fd556654c-jzdsx   1/1     Running   5h18m
# server-deployment-6bc5f558c5-7vpf9   1/1     Running   28s
# server-deployment-6bc5f558c5-cvltl   1/1     Running   28s
# server-deployment-6bc5f558c5-q9j**   1/1     Running   28s
```

**‚úÖ Las 3 r√©plicas est√°n corriendo y compartiendo almacenamiento NFS**

#### Paso 6: Pruebas de persistencia ‚úÖ **COMPLETADO**

Verificaci√≥n de que las 3 r√©plicas comparten los mismos archivos mediante NFS.

**Prueba 1: Subir un archivo**
```bash
# Desde tu m√°quina local, conecta al broker
./clientFileManager 172.31.31.30 32002

# Sube un archivo de prueba
upload archivo_test.txt

# Lista los archivos en el servidor
lls

# Salir
exit
```

**Prueba 2: Verificar persistencia**
```bash
# Vuelve a conectar (puede que te asigne otra r√©plica)
./clientFileManager 172.31.31.30 32002

# El archivo debe seguir ah√≠
lls

# Descarga el archivo para verificar
download archivo_test.txt

# Salir
exit
```

**Prueba 3: Verificar en el servidor NFS**
```bash
# En k8smaster0, verifica que el archivo est√° en el NFS
ls -la /mnt/nfs-filemanager/

# Deber√≠as ver el archivo subido desde el cliente
```

**Prueba 4: Verificar logs de las r√©plicas**
```bash
# Ver logs de cada r√©plica para confirmar que todas est√°n activas
kubectl logs server-deployment-6bc5f558c5-7vpf9
kubectl logs server-deployment-6bc5f558c5-cvltl
kubectl logs server-deployment-6bc5f558c5-q9j**
```

**‚úÖ Ventaja:** Las 3 r√©plicas comparten los mismos archivos mediante NFS. Si una r√©plica falla, las otras siguen sirviendo los mismos datos.

**‚úÖ Resultados de las pruebas:**
```bash
# Crear archivo de prueba
echo "Esto es una prueba" > Prueba.txt

# Primera conexi√≥n - Subir archivo
./clientFileManager 172.31.31.30 32002
> upload Prueba.txt
# Coping file Prueba.txt in to the FileManager path
# Reading file: Prueba.txt 19 bytes

> lls
# Listing files fileManager path
# FileManagerDir/Prueba.txt

# Segunda conexi√≥n - Verificar persistencia
./clientFileManager 172.31.31.30 32002
> lls
# Listing files fileManager path
# FileManagerDir/Prueba.txt  ‚Üê ‚úÖ Archivo persiste entre conexiones
```

**‚úÖ Verificaci√≥n en servidor NFS:**
```bash
# En k8smaster0
ls -la /mnt/nfs-filemanager/
# Prueba.txt  ‚Üê El archivo est√° en el almacenamiento compartido NFS
```

**üéâ CONFIGURACI√ìN NFS EXITOSA - Las 3 r√©plicas comparten datos correctamente**

---

## üîß Comandos √ötiles

### Gesti√≥n del Cluster

```bash
# Ver estado de nodos
kubectl get nodes -o wide

# Ver pods
kubectl get pods -o wide
kubectl get pods -n kube-system

# Ver servicios
kubectl get svc

# Ver eventos
kubectl get events --sort-by='.lastTimestamp'
```

### Gesti√≥n de Deployments

```bash
# Ver deployments
kubectl get deployments

# Describir deployment
kubectl describe deployment broker-deployment
kubectl describe deployment server-deployment

# Ver logs de un pod
kubectl logs <nombre-pod>

# Eliminar deployment
kubectl delete deployment <nombre>

# Aplicar cambios
kubectl apply -f <archivo.yml>

# Escalar r√©plicas
kubectl scale deployment server-deployment --replicas=3
```

### Gesti√≥n de Im√°genes Docker

```bash
# Construir imagen
docker build -t d1n0s/kubernetes-practica2broker:v1 -f DockerfileBroker .
docker build -t d1n0s/kubernetes-practica2server:v2 -f DockerfileServer .

# Subir a Docker Hub
docker login
docker push d1n0s/kubernetes-practica2broker:v1
docker push d1n0s/kubernetes-practica2server:v2

# Ver im√°genes locales
docker images

# Eliminar imagen
docker rmi <imagen>
```

### Gesti√≥n de Nodos

```bash
# Etiquetar nodos
kubectl label nodes k8sslave1.psdi.org node-role=broker
kubectl label nodes k8sslave2.psdi.org node-role=server

# Ver etiquetas
kubectl get nodes --show-labels

# A√±adir nodo worker
cd ~/kub
./kub_addNode.sh <IP>

# Eliminar nodo
kubectl drain <nombre-nodo> --ignore-daemonsets --delete-emptydir-data
kubectl delete node <nombre-nodo>
```

---

## üêõ Troubleshooting

### Problema: Pods en CrashLoopBackOff

**Causa:** Error en la imagen Docker o configuraci√≥n incorrecta.

**Soluci√≥n:**
```bash
# Ver logs del pod
kubectl logs <nombre-pod>

# Describir el pod para ver eventos
kubectl describe pod <nombre-pod>
```

### Problema: No se puede conectar al broker desde fuera del cluster

**Causa:** Security group de AWS bloqueando el puerto 32002.

**Soluci√≥n:**
1. Ve a AWS EC2 Console ‚Üí Security Groups
2. Edita el security group de las instancias
3. A√±ade regla de entrada:
   - Tipo: Custom TCP
   - Puerto: 32002
   - Origen: 0.0.0.0/0

### Problema: Kubernetes no descarga la nueva imagen

**Causa:** Usa la imagen en cach√© con el mismo tag.

**Soluci√≥n:**
```bash
# Cambiar el tag de la imagen
docker build -t d1n0s/kubernetes-practica2server:v3 .
docker push d1n0s/kubernetes-practica2server:v3

# Actualizar el deployment con el nuevo tag
# Editar DeploymentServer.yml: image: ...server:v3
kubectl apply -f DeploymentServer.yml
```

### Problema: Pods no se distribuyen en los nodos deseados

**Causa:** Falta `nodeSelector` o las etiquetas no coinciden.

**Soluci√≥n:**
```bash
# Verificar etiquetas de los nodos
kubectl get nodes --show-labels

# Etiquetar correctamente
kubectl label nodes <nombre-nodo> node-role=<valor> --overwrite

# A√±adir nodeSelector en el deployment
```

### Problema: PVC queda en estado Pending

**Causa:** No hay PV disponible o no coinciden las especificaciones.

**Soluci√≥n:**
```bash
# Ver estado de PV y PVC
kubectl get pv
kubectl get pvc

# Describir para ver el error
kubectl describe pvc <nombre-pvc>

# Verificar que accessModes y storageClassName coinciden
```

---

## üìö Referencias

- [Documentaci√≥n oficial de Kubernetes](https://kubernetes.io/docs/home/)
- [Docker Hub](https://hub.docker.com/)
- [Gu√≠a de vol√∫menes NFS en Kubernetes](https://www.jorgedelacruz.es/2017/12/26/kubernetes-volumenes-nfs/)
- [Calico CNI](https://docs.projectcalico.org/)

---

## ‚úÖ Checklist Final

### Configuraci√≥n B√°sica
- [x] Cluster con 3 nodos (master + 2 workers)
- [x] Im√°genes Docker creadas y subidas
- [x] Broker desplegado en k8sslave1
- [x] Servidor desplegado en k8sslave2
- [x] Servicios NodePort configurados
- [x] Cliente puede conectarse y ejecutar comandos

### Configuraci√≥n Avanzada
- [x] **Config 2: M√∫ltiples nodos con NFS** ‚≠ê **‚úÖ COMPLETADA**
  - [x] Paso 1: Servidor NFS instalado y configurado en k8smaster0 ‚úÖ
  - [x] Paso 2: Cliente NFS instalado en k8sslave2 ‚úÖ
  - [x] Paso 3: PersistentVolume NFS creado ‚úÖ
  - [x] Paso 4: PersistentVolumeClaim NFS vinculado ‚úÖ
  - [x] Paso 5: Deployment con 3 r√©plicas funcionando ‚úÖ
  - [x] Paso 6: Pruebas de persistencia exitosas ‚úÖ

---
